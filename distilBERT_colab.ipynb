{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copia_di_distilBERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "toxic",
      "language": "python",
      "name": "toxic"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPQUdNYEXNG3"
      },
      "source": [
        "# distilBERT <br/>\n",
        "https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpzf5E-tvyW7",
        "outputId": "66a1d027-18fa-42e5-83cc-cb01bdc9e747"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # /content/drive/MyDrive/HLT"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8AvDjNqXokn",
        "outputId": "372614eb-ed02-412d-f013-a6801650ac87"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MCpqtvGXO97",
        "outputId": "a63f1f42-8544-49fa-8a1d-fafdf449c72a"
      },
      "source": [
        "!git clone https://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@github.com/antodima/toxic-comment.git\n",
        "!mv toxic-comment/data/ ."
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'toxic-comment' already exists and is not an empty directory.\n",
            "mv: cannot stat 'toxic-comment/data/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB2hdYnuXNG5"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from sklearn.metrics import accuracy_score, coverage_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import TFDistilBertModel, DistilBertTokenizerFast\n",
        "\n",
        "tf.compat.v1.enable_eager_execution()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_YhB8dZXiqf",
        "outputId": "7b8eb7d3-c9bd-4155-d67d-37fc41855ebe"
      },
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h64jXVxUXNG6"
      },
      "source": [
        "def batch_encode(tokenizer, texts, batch_size=256, max_length=100):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer.batch_encode_plus(batch, max_length=max_length, padding='longest', truncation=True, \n",
        "                                             return_attention_mask=True, return_token_type_ids=False)\n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "        \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTbcLSAXXNG7",
        "outputId": "dbcdfad7-25ab-4819-ca22-c9d6d2df41ad"
      },
      "source": [
        "df = pd.read_csv('data/train.csv')\n",
        "categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "num_classes = len(categories)\n",
        "comments = list(df.comment_text.values)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(df['comment_text'].values, df[categories].values, \n",
        "                                                      train_size=0.8, shuffle=True, random_state=42)\n",
        "print(X_train.shape,y_train.shape)\n",
        "print(X_valid.shape,y_valid.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(127656,) (127656, 6)\n",
            "(31915,) (31915, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0NDKJUKXNG7",
        "outputId": "740ede69-68eb-4e7f-8cb7-1e2af35723ce"
      },
      "source": [
        "df_test = pd.read_csv('data/test.csv')\n",
        "cols = df_test.columns\n",
        "label_cols = list(cols[2:])\n",
        "test_labels_df = pd.read_csv('data/test_labels.csv')\n",
        "df_test = df_test.merge(test_labels_df, on='id', how='left')\n",
        "test_label_cols = list(df_test.columns[2:])\n",
        "df_test = df_test[~df_test[test_label_cols].eq(-1).any(axis=1)] # remove irrelevant rows/comments with -1 values\n",
        "\n",
        "X_test = df_test['comment_text'].values\n",
        "y_test = df_test[categories].values\n",
        "\n",
        "print(X_test.shape,y_test.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(63978,) (63978, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Esq345U3XNG8",
        "outputId": "1dbccf36-1f42-4ae1-8498-c4e4d173d9cd"
      },
      "source": [
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "distilBERT = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "for layer in distilBERT.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'vocab_projector', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2o5DL7qXNG8"
      },
      "source": [
        "max_length = 100\n",
        "# Encode X_train\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, X_train.tolist(), max_length=max_length)\n",
        "# Encode X_valid\n",
        "X_valid_ids, X_valid_attention = batch_encode(tokenizer, X_valid.tolist(), max_length=max_length)\n",
        "# Encode X_test\n",
        "X_test_ids, X_test_attention = batch_encode(tokenizer, X_test.tolist(), max_length=max_length)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX7OL_2FXNG9",
        "outputId": "cab84d6c-a449-4ff2-d476-43ec5028d9d9"
      },
      "source": [
        "input_ids_layer = tf.keras.layers.Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
        "input_attention_layer = tf.keras.layers.Input(shape=(max_length,), name='input_attention', dtype='int32')\n",
        "last_hidden_state = distilBERT([input_ids_layer, input_attention_layer])[0]\n",
        "cls_token = last_hidden_state[:, 0, :]\n",
        "output = tf.keras.layers.Dense(50, activation=\"relu\", kernel_initializer='he_uniform')(cls_token)\n",
        "output = tf.keras.layers.Dropout(0.1)(output)\n",
        "output = tf.keras.layers.Dense(num_classes, activation='sigmoid', name='classifier', kernel_initializer='glorot_uniform')(output)\n",
        "\n",
        "model = tf.keras.Model([input_ids_layer, input_attention_layer], output, name='')\n",
        "model.compile(optimizer='adam', \n",
        "              loss='binary_crossentropy', \n",
        "              metrics=[\n",
        "                  'accuracy',\n",
        "                  'binary_accuracy', \n",
        "                  tf.keras.metrics.AUC(multi_label=True),\n",
        "                  tf.keras.metrics.Precision(),\n",
        "                  tf.keras.metrics.Recall()\n",
        "              ])\n",
        "model.summary()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_attention (InputLayer)    [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model_2 (TFDisti TFBaseModelOutput(la 66362880    input_ids[0][0]                  \n",
            "                                                                 input_attention[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_2 (Sli (None, 768)          0           tf_distil_bert_model_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 50)           38450       tf.__operators__.getitem_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dropout_59 (Dropout)            (None, 50)           0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 6)            306         dropout_59[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 66,401,636\n",
            "Trainable params: 38,756\n",
            "Non-trainable params: 66,362,880\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RY6PuBZXNG9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8d7d44d-6b0f-464e-c4d2-fa61becadcae"
      },
      "source": [
        "mode = 0  # 0: train, 1: evaluate, 2: finetuning\n",
        "\n",
        "if mode == 0:  # training\n",
        "  now = datetime.datetime.now()\n",
        "  ckp_dir = f\"/content/drive/MyDrive/HLT/checkpoint_{now.strftime('%Y%m%d_%H%M%S')}/distil-bert\"\n",
        "  print(f\"Training {ckp_dir} . . .\")\n",
        "  epochs = 4\n",
        "  batch_size = 64\n",
        "  num_steps = len(X_train) // batch_size\n",
        "\n",
        "  with tf.device('/GPU:0'):\n",
        "    history = model.fit(\n",
        "        x = [X_train_ids, X_train_attention],\n",
        "        y = y_train,\n",
        "        epochs = epochs,\n",
        "        batch_size = batch_size,\n",
        "        steps_per_epoch = num_steps,\n",
        "        validation_data = ([X_valid_ids, X_valid_attention], y_valid),\n",
        "        verbose=1,\n",
        "        callbacks=[\n",
        "          ModelCheckpoint(filepath=ckp_dir, save_weights_only=True, monitor='val_binary_accuracy', mode='max', save_best_only=True)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    scores = model.evaluate([X_test_ids, X_test_attention], y_test, verbose=2)\n",
        "    print(f\"Test {model.metrics_names}: {scores}\")\n",
        "    scores = model.evaluate([X_valid_ids, X_valid_attention], y_valid, verbose=2)\n",
        "    print(f\"Validation {model.metrics_names}: {scores}\")\n",
        "\n",
        "elif mode == 1:  # evaluate\n",
        "  path = '/content/drive/MyDrive/HLT/checkpoint_20210920_105809/'\n",
        "  print(f\"Evaluating {path} . . .\")\n",
        "  model.load_weights(tf.train.latest_checkpoint(path))\n",
        "\n",
        "  scores = model.evaluate([X_test_ids, X_test_attention], y_test, verbose=2)\n",
        "  print(f\"Test {model.metrics_names}: {scores}\")\n",
        "  scores = model.evaluate([X_valid_ids, X_valid_attention], y_valid, verbose=2)\n",
        "  print(f\"Validation {model.metrics_names}: {scores}\")\n",
        "\n",
        "elif mode == 2:  # fnetuning\n",
        "  path = '/content/drive/MyDrive/HLT/checkpoints_9751/'\n",
        "  model.load_weights(tf.train.latest_checkpoint(path))\n",
        "  \n",
        "  scores = model.evaluate([X_test_ids, X_test_attention], y_test, verbose=2)\n",
        "  print(f\"Test {model.metrics_names} (pre): {scores}\")\n",
        "  scores = model.evaluate([X_valid_ids, X_valid_attention], y_valid, verbose=2)\n",
        "  print(f\"Validation {model.metrics_names} (pre): {scores}\")\n",
        "\n",
        "  print(f\"Finetuning {path} . . .\")\n",
        "  for layer in model.layers[-3].layers:\n",
        "    layer.trainable = True\n",
        "  model.summary()\n",
        "  epochs = 2\n",
        "  batch_size = 64\n",
        "  num_steps = len(X_train) // batch_size\n",
        "\n",
        "  now = datetime.datetime.now()\n",
        "  ckp_dir = f\"/content/drive/MyDrive/HLT/checkpoint_ft_{now.strftime('%Y%m%d_%H%M%S')}/distil-bert-finetuned\"\n",
        "\n",
        "  with tf.device('/GPU:0'):\n",
        "    history = model.fit(\n",
        "        x = [X_train_ids, X_train_attention],\n",
        "        y = y_train,\n",
        "        epochs = epochs,\n",
        "        batch_size = batch_size,\n",
        "        steps_per_epoch = num_steps,\n",
        "        validation_data = ([X_valid_ids, X_valid_attention], y_valid),\n",
        "        verbose=1,\n",
        "        callbacks=[\n",
        "          ModelCheckpoint(filepath=ckp_dir, save_weights_only=True, monitor='val_binary_accuracy', mode='max', save_best_only=True)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  scores = model.evaluate([X_test_ids, X_test_attention], y_test, verbose=2)\n",
        "  print(f\"Test {model.metrics_names}: {scores}\")\n",
        "  scores = model.evaluate([X_valid_ids, X_valid_attention], y_valid, verbose=2)\n",
        "  print(f\"Validation {model.metrics_names}: {scores}\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training /content/drive/MyDrive/HLT/checkpoint_20210920_195726/distil-bert . . .\n",
            "Epoch 1/4\n",
            "1994/1994 [==============================] - 986s 490ms/step - loss: 0.0729 - accuracy: 0.8433 - binary_accuracy: 0.9751 - auc_2: 0.9304 - precision_2: 0.7613 - recall_2: 0.4654 - val_loss: 0.0607 - val_accuracy: 0.9941 - val_binary_accuracy: 0.9779 - val_auc_2: 0.9565 - val_precision_2: 0.7589 - val_recall_2: 0.5888\n",
            "Epoch 2/4\n",
            "1994/1994 [==============================] - 975s 489ms/step - loss: 0.0621 - accuracy: 0.9489 - binary_accuracy: 0.9777 - auc_2: 0.9573 - precision_2: 0.7894 - recall_2: 0.5315 - val_loss: 0.0589 - val_accuracy: 0.9941 - val_binary_accuracy: 0.9785 - val_auc_2: 0.9576 - val_precision_2: 0.8195 - val_recall_2: 0.5358\n",
            "Epoch 3/4\n",
            "1994/1994 [==============================] - 974s 489ms/step - loss: 0.0602 - accuracy: 0.9558 - binary_accuracy: 0.9782 - auc_2: 0.9592 - precision_2: 0.7987 - recall_2: 0.5405 - val_loss: 0.0576 - val_accuracy: 0.9931 - val_binary_accuracy: 0.9785 - val_auc_2: 0.9620 - val_precision_2: 0.8388 - val_recall_2: 0.5178\n",
            "Epoch 4/4\n",
            "1994/1994 [==============================] - 973s 488ms/step - loss: 0.0594 - accuracy: 0.9717 - binary_accuracy: 0.9784 - auc_2: 0.9604 - precision_2: 0.8010 - recall_2: 0.5468 - val_loss: 0.0569 - val_accuracy: 0.9889 - val_binary_accuracy: 0.9791 - val_auc_2: 0.9567 - val_precision_2: 0.8315 - val_recall_2: 0.5441\n",
            "2000/2000 - 386s - loss: 0.0790 - accuracy: 0.9914 - binary_accuracy: 0.9713 - auc_2: 0.9411 - precision_2: 0.6544 - recall_2: 0.5109\n",
            "Test ['loss', 'accuracy', 'binary_accuracy', 'auc_2', 'precision_2', 'recall_2']: [0.07895965129137039, 0.991387665271759, 0.9713393449783325, 0.941145122051239, 0.6544442772865295, 0.5108980536460876]\n",
            "998/998 - 193s - loss: 0.0569 - accuracy: 0.9889 - binary_accuracy: 0.9791 - auc_2: 0.9567 - precision_2: 0.8315 - recall_2: 0.5441\n",
            "Validation ['loss', 'accuracy', 'binary_accuracy', 'auc_2', 'precision_2', 'recall_2']: [0.05693058669567108, 0.9888767004013062, 0.9790856242179871, 0.9567497372627258, 0.831497073173523, 0.5441051721572876]\n"
          ]
        }
      ]
    }
  ]
}