{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distilBERT <br/>\n",
    "https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install transformers\n",
    "!pip install -U scikit-learn\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TFDistilBertModel, DistilBertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_encode(tokenizer, texts, batch_size=256, max_length=100):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    A function that encodes a batch of texts and returns the texts'\n",
    "    corresponding encodings and attention masks that are ready to be fed \n",
    "    into a pre-trained transformer model.\n",
    "    \n",
    "    Input:\n",
    "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
    "        - texts:       List of strings where each string represents a text\n",
    "        - batch_size:  Integer controlling number of texts in a batch\n",
    "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
    "    Output:\n",
    "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
    "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer.batch_encode_plus(batch, max_length=max_length, padding='longest', truncation=True, \n",
    "                                             return_attention_mask=True, return_token_type_ids=False)\n",
    "        input_ids.extend(inputs['input_ids'])\n",
    "        attention_mask.extend(inputs['attention_mask'])\n",
    "        \n",
    "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127656,) (127656, 6)\n",
      "(31915,) (31915, 6)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "num_classes = len(categories)\n",
    "comments = list(df.comment_text.values)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(df['comment_text'].values, df[categories].values, \n",
    "                                                      train_size=0.8, shuffle=True, random_state=42)\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_valid.shape,y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63978,) (63978, 6)\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('data/test.csv')\n",
    "cols = df_test.columns\n",
    "label_cols = list(cols[2:])\n",
    "test_labels_df = pd.read_csv('data/test_labels.csv')\n",
    "df_test = df_test.merge(test_labels_df, on='id', how='left')\n",
    "test_label_cols = list(df_test.columns[2:])\n",
    "df_test = df_test[~df_test[test_label_cols].eq(-1).any(axis=1)] # remove irrelevant rows/comments with -1 values\n",
    "\n",
    "X_test = df_test['comment_text'].values\n",
    "y_test = df_test[categories].values\n",
    "\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'activation_13', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "distilBERT = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "for layer in distilBERT.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode X_train\n",
    "X_train_ids, X_train_attention = batch_encode(tokenizer, X_train.tolist())\n",
    "# Encode X_valid\n",
    "X_valid_ids, X_valid_attention = batch_encode(tokenizer, X_valid.tolist())\n",
    "# Encode X_test\n",
    "X_test_ids, X_test_attention = batch_encode(tokenizer, X_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "\n",
    "input_ids_layer = tf.keras.layers.Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
    "input_attention_layer = tf.keras.layers.Input(shape=(max_length,), name='input_attention', dtype='int32')\n",
    "last_hidden_state = distilBERT([input_ids_layer, input_attention_layer])[0]\n",
    "cls_token = last_hidden_state[:, 0, :]\n",
    "output = tf.keras.layers.Dense(num_classes, activation='softmax')(cls_token)\n",
    "model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n",
    "model.compile(tf.keras.optimizers.Adam(lr=lr), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "batch_size = 64\n",
    "num_steps = len(X_train) // batch_size\n",
    "\n",
    "history = model.fit(\n",
    "    x = [X_train_ids, X_train_attention],\n",
    "    y = y_train,\n",
    "    epochs = epochs,\n",
    "    batch_size = batch_size,\n",
    "    steps_per_epoch = num_steps,\n",
    "    validation_data = ([X_valid_ids, X_valid_attention], y_valid),\n",
    "    verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic",
   "language": "python",
   "name": "toxic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
