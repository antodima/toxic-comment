{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepESN <br/>\n",
    "https://github.com/gallicch/DeepRC-TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Input, Reshape, Dropout, Bidirectional, LSTM, GRU\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, multilabel_confusion_matrix, plot_confusion_matrix, classification_report, coverage_error\n",
    "\n",
    "from DeepRC import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df['comment_text'] = df['comment_text'].map(lambda com : clean_text(com))\n",
    "\n",
    "categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "num_classes = len(categories)\n",
    "train, valid = train_test_split(df, random_state=42, test_size=0.33, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values:  False\n",
      "Same columns between train and test:  False\n",
      "Number of rows original: 153164\n",
      "Number of rows after: 63978\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>one_hot_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>Thank you for understanding. I think very high...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0002f87b16116a7f</td>\n",
       "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0003e1cccfd5a40a</td>\n",
       "      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00059ace3e3e9a53</td>\n",
       "      <td>\" \\n\\n == Before adding a new product to the l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                       comment_text  \\\n",
       "5   0001ea8717f6de06  Thank you for understanding. I think very high...   \n",
       "7   000247e83dcc1211                   :Dear god this site is horrible.   \n",
       "11  0002f87b16116a7f  \"::: Somebody will invariably try to add Relig...   \n",
       "13  0003e1cccfd5a40a  \" \\n\\n It says it right there that it IS a typ...   \n",
       "14  00059ace3e3e9a53  \" \\n\\n == Before adding a new product to the l...   \n",
       "\n",
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "5       0             0        0       0       0              0   \n",
       "7       0             0        0       0       0              0   \n",
       "11      0             0        0       0       0              0   \n",
       "13      0             0        0       0       0              0   \n",
       "14      0             0        0       0       0              0   \n",
       "\n",
       "        one_hot_labels  \n",
       "5   [0, 0, 0, 0, 0, 0]  \n",
       "7   [0, 0, 0, 0, 0, 0]  \n",
       "11  [0, 0, 0, 0, 0, 0]  \n",
       "13  [0, 0, 0, 0, 0, 0]  \n",
       "14  [0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('data/test.csv')\n",
    "df['comment_text'] = df['comment_text'].map(lambda com : clean_text(com))\n",
    "cols = df_test.columns\n",
    "label_cols = list(cols[2:])\n",
    "\n",
    "test_labels_df = pd.read_csv('data/test_labels.csv')\n",
    "df_test = df_test.merge(test_labels_df, on='id', how='left')\n",
    "test_label_cols = list(df_test.columns[2:])\n",
    "print('Null values: ', df_test.isnull().values.any()) #should not be any null sentences or labels\n",
    "print('Same columns between train and test: ', label_cols == test_label_cols) #columns should be the same\n",
    "print(f\"Number of rows original: {df_test.shape[0]}\")\n",
    "\n",
    "df_test = df_test[~df_test[test_label_cols].eq(-1).any(axis=1)] #remove irrelevant rows/comments with -1 values\n",
    "df_test['one_hot_labels'] = list(df_test[test_label_cols].values)\n",
    "print(f\"Number of rows after: {df_test.shape[0]}\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set (106912, 10, 1) (106912, 6)\n",
      "validation set (52659, 10, 1) (52659, 6)\n",
      "test set (63978, 10) (63978, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train = train.comment_text\n",
    "y_train = train[categories]\n",
    "X_valid = valid.comment_text\n",
    "y_valid = valid[categories]\n",
    "X_test = df_test.comment_text.values\n",
    "y_test = df_test[categories].values\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_matrix(X_train, mode='tfidf')\n",
    "X_valid = tokenizer.texts_to_matrix(X_valid, mode='tfidf')\n",
    "X_test = tokenizer.texts_to_matrix(X_test, mode='tfidf')\n",
    "\n",
    "X_train = tf.expand_dims(X_train, axis=2)\n",
    "X_valid = tf.expand_dims(X_valid, axis=2)\n",
    "\n",
    "y_train = y_train.values\n",
    "y_valid = y_valid.values\n",
    "\n",
    "print('training set',X_train.shape,y_train.shape)\n",
    "print('validation set',X_valid.shape,y_valid.shape)\n",
    "print('test set',X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DeepESN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 10, 1)]           0         \n",
      "_________________________________________________________________\n",
      "reservoir (SimpleDeepReservo (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "readout (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 25,456\n",
      "Trainable params: 25,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "reservoir_units = 200\n",
    "batch_size = 128\n",
    "\n",
    "inputs = Input(shape=(X_train.shape[1],1), name='inputs')\n",
    "reservoir = SimpleDeepReservoirLayer(units=reservoir_units, layers=10, trainable=False, name='reservoir')(inputs)\n",
    "\n",
    "#reservoir = Reshape((1,reservoir_units), input_shape=(reservoir_units,))(reservoir)\n",
    "#readout = Bidirectional(GRU(10), merge_mode='sum')(reservoir)\n",
    "\n",
    "readout = Dense(100, activation='relu')(reservoir)\n",
    "readout = Dropout(0.1)(readout)\n",
    "readout = Dense(50, activation='relu')(readout)\n",
    "readout = Dropout(0.1)(readout)\n",
    "readout = Dense(num_classes, activation='sigmoid', name='readout')(readout)\n",
    "\n",
    "model = Model(inputs, readout, name='DeepESN')\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=[\n",
    "                    'accuracy',\n",
    "                    'binary_accuracy', \n",
    "                    tf.keras.metrics.AUC(multi_label=True),\n",
    "                    tf.keras.metrics.Precision(),\n",
    "                    tf.keras.metrics.Recall()\n",
    "              ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836/836 [==============================] - 40s 45ms/step - loss: 0.1660 - accuracy: 0.6692 - binary_accuracy: 0.9496 - auc_39: 0.6040 - precision_39: 0.0430 - recall_39: 0.0150 - val_loss: 0.1308 - val_accuracy: 0.9937 - val_binary_accuracy: 0.9629 - val_auc_39: 0.7145 - val_precision_39: 0.4759 - val_recall_39: 0.0076\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=1, verbose=1, validation_data=(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 - 30s - loss: 0.1363 - accuracy: 0.9973 - binary_accuracy: 0.9621 - auc_39: 0.6779 - precision_39: 0.3964 - recall_39: 0.0046\n",
      "Test ['loss', 'accuracy', 'binary_accuracy', 'auc_39', 'precision_39', 'recall_39']: [0.13627831637859344, 0.9973115921020508, 0.9621403813362122, 0.6779302954673767, 0.3964497148990631, 0.004621326923370361]\n",
      "1646/1646 - 25s - loss: 0.1308 - accuracy: 0.9937 - binary_accuracy: 0.9629 - auc_39: 0.7145 - precision_39: 0.4759 - recall_39: 0.0076\n",
      "Validation ['loss', 'accuracy', 'binary_accuracy', 'auc_39', 'precision_39', 'recall_39']: [0.13083255290985107, 0.993733286857605, 0.9629223942756653, 0.7145265936851501, 0.47593581676483154, 0.007602938450872898]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test {model.metrics_names}: {scores}\")\n",
    "scores = model.evaluate(X_valid, y_valid, verbose=2)\n",
    "print(f\"Validation {model.metrics_names}: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation scores:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        toxic       0.48      0.02      0.03      5083\n",
      " severe_toxic       0.00      0.00      0.00       526\n",
      "      obscene       0.00      0.00      0.00      2831\n",
      "       threat       0.00      0.00      0.00       152\n",
      "       insult       0.00      0.00      0.00      2643\n",
      "identity_hate       0.00      0.00      0.00       471\n",
      "\n",
      "    micro avg       0.48      0.01      0.01     11706\n",
      "    macro avg       0.08      0.00      0.01     11706\n",
      " weighted avg       0.21      0.01      0.01     11706\n",
      "  samples avg       0.00      0.00      0.00     11706\n",
      "\n",
      "Test scores:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        toxic       0.40      0.01      0.02      6090\n",
      " severe_toxic       0.00      0.00      0.00       367\n",
      "      obscene       0.00      0.00      0.00      3691\n",
      "       threat       0.00      0.00      0.00       211\n",
      "       insult       0.00      0.00      0.00      3427\n",
      "identity_hate       0.00      0.00      0.00       712\n",
      "\n",
      "    micro avg       0.40      0.00      0.01     14498\n",
      "    macro avg       0.07      0.00      0.00     14498\n",
      " weighted avg       0.17      0.00      0.01     14498\n",
      "  samples avg       0.00      0.00      0.00     14498\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anto/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anto/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anto/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anto/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anto/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anto/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_valid_pred = model.predict(X_valid)\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_valid_pred = tf.where(y_valid_pred >= 0.5, 1, 0)\n",
    "y_test_pred = tf.where(y_test_pred >= 0.5, 1, 0)\n",
    "\n",
    "print(f\"Validation scores:\\n{classification_report(y_valid, y_valid_pred, target_names=categories)}\")\n",
    "print(f\"Test scores:\\n{classification_report(y_test, y_test_pred, target_names=categories)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y, y_hat in zip(y_valid, y_valid_pred):\n",
    "    print(y, y_hat.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic",
   "language": "python",
   "name": "toxic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
